{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6XZ38XqxaoYntbZ286l7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llacuesta/nvidia-python/blob/main/01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: verify CUDA before running\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "bp2fOq84egbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba==0.58.0"
      ],
      "metadata": {
        "id": "rVGdfTJoj3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: verify numba\n",
        "import numba\n",
        "print(numba.__version__)"
      ],
      "metadata": {
        "id": "Waf-JuFnjrn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "print(cuda.gpus)"
      ],
      "metadata": {
        "id": "yN_MvSYnk6qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro To CUDA Python with Numba**"
      ],
      "metadata": {
        "id": "V6cD6QSkdWn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Numba\n",
        "- accelarating numerically focused Python for CPUs or GPUs\n",
        "- function compiler, compiled functions only instead of full apps\n",
        "- type-specialized instead of generic data types\n",
        "- just-in-time, translating functions when they are first called for use in Jupyter notebooks\n",
        "- numerically-focused using NumPy"
      ],
      "metadata": {
        "id": "xsZXPCwGD0Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling for CPU\n",
        "- Numba is enabled using `@jit` decorator\n",
        "- Numba also saves the function implementation in the `.py_func` attribute"
      ],
      "metadata": {
        "id": "WqfX-TuPH73s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import jit\n",
        "import math\n",
        "\n",
        "# function decorator for jit\n",
        "@jit\n",
        "def hypot(x, y):\n",
        "  x = abs(x);\n",
        "  y = abs(y);\n",
        "  t = min(x, y);\n",
        "  x = max(x, y);\n",
        "  t = t / x;\n",
        "  return x * math.sqrt(1 + t * t)"
      ],
      "metadata": {
        "id": "S_bqnQU4EqRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypot(3.0, 4.0)\n",
        "hypot.py_func(3.0, 4.0) # same as above"
      ],
      "metadata": {
        "id": "qlBlkiaKIWUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking"
      ],
      "metadata": {
        "id": "p0sk0l14dvi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- can be done with Python's `timeit`\n",
        "- Numba is typically faster than pure Python implementation\n",
        "- Python's built-in functions (`math`) are typically faster than Numba due to overhead"
      ],
      "metadata": {
        "id": "yhvkDPypJj3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit hypot.py_func(3.0, 4.0)"
      ],
      "metadata": {
        "id": "3Cd3IXtMRVnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit hypot(3.0, 4.0)"
      ],
      "metadata": {
        "id": "RczHK0UpRh2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit math.hypot(3.0, 4.0)"
      ],
      "metadata": {
        "id": "jiaue9XsRlXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Numba Works"
      ],
      "metadata": {
        "id": "AST460Yddza7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Numba Works\n",
        "- Numba wraps the Python function, bytecode analysis is done on the function, and the types of the arguments are inferred\n",
        "- types are important as certain GPUs can run very differently based on the data types\n",
        "- sometimes Numba cannot translate a function (such as dictionaries), Numba wrapped functions will still run by falling back to `object mode` that does not use type-specialization\n",
        "- to show errors, `nopython mode` can be forced using `nopython=True` argument to the `@jit` decorator\n",
        "- `nopython mode` is recommended to maximize `jit` performance"
      ],
      "metadata": {
        "id": "itQ5x1njTtek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypot.inspect_types()"
      ],
      "metadata": {
        "id": "RXSwpB_qUREE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# object mode\n",
        "# note: looks like by default, jit is in nopython mode\n",
        "@jit\n",
        "def cannot_compile(x):\n",
        "    return x['key']\n",
        "\n",
        "cannot_compile(dict(key='value'))"
      ],
      "metadata": {
        "id": "gWhzYzXwVPST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nopython mode\n",
        "@jit(nopython=False)\n",
        "def cannot_compile(x):\n",
        "    return x['key']\n",
        "\n",
        "cannot_compile(dict(key='value'))"
      ],
      "metadata": {
        "id": "m68NpaGcVUuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making `ufuncs` for the GPU"
      ],
      "metadata": {
        "id": "Z23Y0yNVd607"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ufuncs or universal functions are functions (in NumPy) that can take arguments of varying dimensions and operate on them on a per-element basis\n",
        "- using `@vectorize` decorator, a ufunc can be optimized\n",
        "\n",
        "(cont.)"
      ],
      "metadata": {
        "id": "rWhLpyAuV_0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import vectorize\n",
        "\n",
        "@vectorize\n",
        "def add_ten(num):\n",
        "  return num + 10"
      ],
      "metadata": {
        "id": "xqAEeNDOZJmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([10, 20, 30, 40])\n",
        "c = np.arange(4*4).reshape((4,4))\n",
        "\n",
        "nums = np.arange(10)\n",
        "add_ten(a)"
      ],
      "metadata": {
        "id": "v6CjEozkZQsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(cont.)\n",
        "\n",
        "- to use CUDA on the GPU, set the `target` attribute of `@vectorize` to `'cuda'`, and provide explicit type signatures to th arguments. See [Numba available types](https://numba.pydata.org/numba-doc/dev/reference/types.html)\n",
        "- this will:\n",
        "  - compile a CUDA kernet to execute the ufunc in parallel over the input elements\n",
        "  - allocate GPU memory for inputs and outputs\n",
        "  -  execute the CUDA kernel and copy the result back from the GPU to CPU"
      ],
      "metadata": {
        "id": "hNPX2u0NZjkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@vectorize(['int64(int64, int64)'], target='cuda') # Type signature and target are required for the GPU\n",
        "def add_ufunc(x, y):\n",
        "    return x + y"
      ],
      "metadata": {
        "id": "7SZkGRpIctQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some considerations are discussed before using the GPU:\n",
        "- GPU needs large inputs and more complex operations to compensate for the overhead due to parallelism\n",
        "- copying data to and from the GPU can be expensive, keep the data there until processing complete unless otherwise necessary\n",
        "- data types must be correct, use exact data types instead of larger data types (float64 > float32 > int64 > int32)\n",
        "- not all Python is allowed in the GPU. Only the following are allowed:\n",
        "  - `if`/`elif`/`else`\n",
        "  - `while` and `for` loops\n",
        "  - Basic math operators\n",
        "  - Selected functions from the `math` and `cmath` modules\n",
        "  - Tuples"
      ],
      "metadata": {
        "id": "8RRzqnMLfFe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit np.add(b, c)   # NumPy on CPU"
      ],
      "metadata": {
        "id": "RV3vVUfLheqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc(b, c) # Numba on GPU"
      ],
      "metadata": {
        "id": "8PSnBV12hjAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: For functions that does not perform element-wise operations, use `cuda.jit`"
      ],
      "metadata": {
        "id": "2nJcfTTNTa-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def polar_to_cartesian(rho, theta):\n",
        "    x = rho * math.cos(theta)\n",
        "    y = rho * math.sin(theta)\n",
        "    return x, y\n",
        "\n",
        "@vectorize(['float32(float32, float32, float32, float32)'], target='cuda')\n",
        "def polar_distance(rho1, theta1, rho2, theta2):\n",
        "    x1, y1 = polar_to_cartesian(rho1, theta1) # We can use device functions inside our GPU ufuncs\n",
        "    x2, y2 = polar_to_cartesian(rho2, theta2)\n",
        "\n",
        "    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5"
      ],
      "metadata": {
        "id": "xpsg3e4tTlE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Drivers"
      ],
      "metadata": {
        "id": "kEELC4M2-Pql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- using `cuda` from Numba, communication and transfer can be reduced by copying inputs to the GPU once instead of repeated to and from\n",
        "- we can do this using CUDA device arrays to hold data instead of passing data directly\n"
      ],
      "metadata": {
        "id": "aSIH69ZD-i1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@vectorize(['float32(float32, float32)'], target='cuda')\n",
        "def add_ufunc(x, y):\n",
        "    return x + y\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "\n",
        "# use device arrays\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "\n",
        "print(x_device)\n",
        "print(x_device.shape)\n",
        "print(x_device.dtype)"
      ],
      "metadata": {
        "id": "-12dQr0CUU4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc(x, y)"
      ],
      "metadata": {
        "id": "nqyySb5dUc-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit add_ufunc(x_device, y_device) # no copy overhead"
      ],
      "metadata": {
        "id": "0FYg_HLBUdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To copy output from GPU to host, use the `out` keyword buffer."
      ],
      "metadata": {
        "id": "tUe7MNXMUgzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_device = cuda.device_array(shape=(n,), dtype=np.float32) # creating a device array, does not initialize unline numpy\n",
        "\n",
        "add_ufunc(x_device, y_device, out=out_device) # set out output buffer\n",
        "out_host = out_device.copy_to_host() # copy to host\n",
        "print(out_host[:10])"
      ],
      "metadata": {
        "id": "n3E727MOUsB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}